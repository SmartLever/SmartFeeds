{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Video to Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from tools import download_mp3 , chunk_text,divide_audio, divide_text, get_transcript, get_video_details\n",
    "from tools import get_recent_podcasts\n",
    "\n",
    "\n",
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# set API key for OpenAI\n",
    "openai.api_key =config['openai']['api_key']\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['openai']['api_key']\n",
    "\n",
    "dir_save = 'temp'\n",
    "\n",
    "# check if exist dir_save\n",
    "if not os.path.exists(dir_save):\n",
    "    os.makedirs(dir_save)\n",
    "\n",
    "# Create temp folder for programs\n",
    "name = 'notebook'\n",
    "dir_save_program = os.path.join(dir_save,name)\n",
    "if not os.path.exists(dir_save_program):\n",
    "    os.makedirs(dir_save_program)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_to_docs(transcript):\n",
    "    no_estaba = True\n",
    "    # Summarize text with OpenAI and langchain\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(transcript['link'], add_video_info=False,language='en')\n",
    "        data = loader.load()\n",
    "    except:\n",
    "        loader = YoutubeLoader.from_youtube_url(transcript['link'], add_video_info=False,language='es')\n",
    "        data = loader.load()\n",
    "\n",
    "    # add info \n",
    "    file_save = os.path.join(dir_save_program,transcript['title']+'.'+transcript['format'])\n",
    "    file_save_txt = os.path.join(dir_save_program,transcript['title']+'.txt')\n",
    "    transcript['transcript'] = data[0].page_content\n",
    "    transcript['file_save_txt'] = file_save_txt\n",
    "\n",
    "    # save text\n",
    "    if len(transcript['transcript'])>0 and not os.path.exists(transcript['file_save_txt']):\n",
    "        with open(transcript['file_save_txt'], 'w') as f:\n",
    "            f.write(transcript['transcript'])\n",
    "            print(f'File {transcript[\"file_save_txt\"],} saved')    \n",
    "    else:\n",
    "        print(f'File {transcript[\"file_save_txt\"],} already exist')\n",
    "        no_estaba = False\n",
    "\n",
    "    print( transcript['transcript'][-100:])\n",
    "\n",
    "    ## split text into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0) #default 4000\n",
    "    return text_splitter.split_documents(data), no_estaba\n",
    "\n",
    "\n",
    "def summarize_docs(docs, output_language='en'):\n",
    "    # Summarize text with OpenAI and langchain\n",
    "    if output_language == 'es':\n",
    "        prompt_map= \"\"\" \n",
    "                Resumen en español el texto.\\\n",
    "                Organiza el resumen separando ideas y conceptos clave con guiones. \\\n",
    "                Asegurate de incluir datos, números y nombres relevantes.  \\\n",
    "                El resultado seguirá un formato similar al siguiente:        \n",
    "                    [* Idea/Concepto clave 1  (dato o número relevante, nombre relacionado)]\\\n",
    "                    [* Idea/Concepto clave 2  (dato o número relevante, nombre relacionado)]\\\n",
    "                    [* Idea/Concepto clave 3  (dato o número relevante, nombre relacionado)]\\\n",
    "                    \n",
    "                text  :\n",
    "                    {text}    \n",
    "                    \"\"\"\n",
    "        PROMPT_MAP = PromptTemplate(template=prompt_map, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "        prompt_combine = \"\"\"\n",
    "                Eres un redactor con 10 años de experiencia.\\\n",
    "                Tu trabajo es mejorar, organizar, limpiar y traducir en español el siguiente texto.\\\n",
    "                El texto tiene cada idea o concepto clave separado por guiones o puntos, une en un mismo guión las ideas o conceptos clave que sean similares.\\\n",
    "                El resultado seguirá un formato similar al siguiente:        \n",
    "                        [* Idea/Concepto clave 1  (dato o número relevante, nombre relacionado)]\\\n",
    "                        [* Idea/Concepto clave 2  (dato o número relevante, nombre relacionado)]\\\n",
    "                        [* Idea/Concepto clave 3  (dato o número relevante, nombre relacionado)]\\\n",
    "                texto : {text}\"\"\"\n",
    "    else:\n",
    "        prompt_map= \"\"\" \n",
    "                Summary in English of the text.\\\n",
    "                Organize the summary by separating key ideas and concepts with dashes.\\\n",
    "                Make sure to include relevant data, numbers, and names.\\\n",
    "                The result will follow a similar format to the following:\\\n",
    "                [* Key Idea/Concept 1 (relevant data or number, related name)]\\\n",
    "                [* Key Idea/Concept 2 (relevant data or number, related name)]\\\n",
    "                [* Key Idea/Concept 3 (relevant data or number, related name)]\\\n",
    "\n",
    "t               text :\n",
    "                    {text}    \n",
    "                    \"\"\"\n",
    "        PROMPT_MAP = PromptTemplate(template=prompt_map, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "        prompt_combine = \"\"\"\n",
    "                You are a writer with 10 years of experience. \\\n",
    "                Your task is to improve, organize, clean, and translate the following text into English. \\\n",
    "                The text has each key idea or concept separated by dashes or dots.\\\n",
    "                  Combine similar key ideas or concepts with the same dash. The result will follow a similar format to the following:\\\n",
    "                [* Key Idea/Concept 1 (relevant data or number, related name)]\\\n",
    "                [* Key Idea/Concept 2 (relevant data or number, related name)]\\\n",
    "                [* Key Idea/Concept 3 (relevant data or number, related name)]\\\n",
    "\n",
    "                text: {text}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    PROMPT_COMBINE= PromptTemplate(template=prompt_combine, input_variables=[\"text\"])\n",
    "    ## use te summarize chain, \"refine\" type\n",
    "    model_llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)# type: ignore\n",
    "    chain = load_summarize_chain(model_llm, chain_type=\"map_reduce\", return_intermediate_steps=True, \n",
    "                                map_prompt=PROMPT_MAP, combine_prompt=PROMPT_COMBINE) \n",
    "    return chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "File ('temp/notebook/AMA #7: Cold Exposure, Maximizing REM Sleep & My Next Scientific Studies.txt',) saved\n",
      "at's\n",
      "hubermanlab.com/premium. And as always, thank you for\n",
      "your interest in science. [MUSIC PLAYING]\n",
      "Summarizing AMA #7: Cold Exposure, Maximizing REM Sleep & My Next Scientific Studies 2023-05-31 12:00:26\n",
      "- Andrew Huberman introduces the Huberman Lab podcast and its premium subscriber channel, which supports research on mental and physical health and performance. The premium channel costs $10 per month or $100 for the year and provides access to AMAs, transcripts, and exclusive content. The Tiny Foundation will match all funds raised for research through the premium channel.\n",
      "- Deliberate cold exposure can increase the release and production of immune molecules and cells. A study showed that repeated deliberate cold exposure over six weeks can lead to trends towards increased immune system markers. The molecules epinephrine and norepinephrine released during deliberate cold exposure and hyperventilation can have pro-immune effects in the short term but can suppress the immune system if elevated chronically. Deliberate cold exposure and cyclic hyperventilation can both cause the deployment of immune molecules if done repeatedly. He mentions that deliberate cold exposure can increase immune system markers, but provides cautionary advice and links to studies for further information. He advises against deliberate cold exposure if someone is already feeling sick, but if they insist on doing it, they should warm up afterwards and not expose themselves to stressful heat. If feeling good, do deliberate cold exposure and warm up afterwards. If feeling rundown, still do deliberate cold exposure but warm up well afterwards. If feeling sick, rest and avoid stressful or challenging activities.\n",
      "- When we have a viral or bacterial infection, sickness circuits in the brain get activated, encouraging us to be in the fetal position and move less to help us heal. Cold can boost the immune system under certain conditions but can also deplete it under others. Nasal breathing is better than mouth breathing, especially during low-level cardio, as the mouth is a main site of entry for infections.\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Read program names and rss urls\n",
    "output_language = 'en' #en\n",
    "program_to_rss_url = {'Despegamos': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCfYk6FdS8bFLrOz_n0b0YTQ\",\n",
    "                      'Juan Rallo': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCBLCvUUCiSqBCEc-TqZ9rGw\",\n",
    "                      'Marc Vidal': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCKC77AR_zWXRTE2GOD_2Uag\",\n",
    "                      'Huberman Lab': \"https://www.youtube.com/feeds/videos.xml?channel_id=UC2D2CMWXMOVWx7giW1n3LIg\",\n",
    "                      'Crea y Transforma': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCs0QXHkbkBDM_NwnRiGFoTA\",\n",
    "                      'Niko Garnier': \"https://www.youtube.com/feeds/videos.xml?channel_id=UClGDJkOcXBLA3qENf4CMcnw\",\n",
    "                      'The Rich Dad Channel': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCuifm5ns5SRG8LZJ6gCfKyw\",\n",
    "                     'TheDavidLin': \"https://www.youtube.com/feeds/videos.xml?channel_id=UClBMLpP3UHXLmgEypMmXPuA\",\n",
    "                      'Bloomberg The Open': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCIALMKvObZNtJ6AmdCLP7Lg\",\n",
    "                      'Balance of Power': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCIALMKvObZNtJ6AmdCLP7Lg\",\n",
    "                      'Bloomberg Real Yield': \"https://www.youtube.com/feeds/videos.xml?channel_id=UCIALMKvObZNtJ6AmdCLP7Lg\",\n",
    "                      'Bloomberg Surveillance':\"https://www.youtube.com/feeds/videos.xml?channel_id=UCIALMKvObZNtJ6AmdCLP7Lg\"\n",
    "                      }\n",
    "# fill if you only want to download some programs of the channel\n",
    "name_to_program = {'Despegamos': 'Despegamos', 'Bloomberg The Open':'Bloomberg The Open',\n",
    "                       'Bloomberg Real Yield':'Bloomberg Real Yield',\n",
    "                       'Balance of Power':'Balance of Power',\n",
    "                       'Bloomberg Surveillance':'Bloomberg Surveillance'}\n",
    "\n",
    "for p in program_to_rss_url.keys():\n",
    "    if 'youtube' in program_to_rss_url[p] and p not in name_to_program.keys():\n",
    "        name_to_program[p] = ''\n",
    "\n",
    "\n",
    "is_link = False\n",
    "for name in program_to_rss_url.keys():\n",
    "    if is_link:\n",
    "        link = 'https://www.youtube.com/watch?v=OJh8OoAVe9Q'\n",
    "        programs =  get_recent_podcasts(link, is_link=True)\n",
    "    else:\n",
    "        programs = get_recent_podcasts(program_to_rss_url[name], program_name = name_to_program[name])\n",
    "\n",
    "\n",
    "    ## Youtube Loader\n",
    "\n",
    "    print(len(programs))\n",
    "    if len(programs)>0:\n",
    "        try:\n",
    "            transcript = programs[0]\n",
    "            docs,no_estaba =transcript_to_docs(transcript)\n",
    "        except:\n",
    "            transcript = programs[1]\n",
    "            docs,no_estaba = transcript_to_docs(transcript)\n",
    "        if no_estaba:\n",
    "            print(f'Summarizing {transcript[\"title\"]} {transcript[\"published\"]}')\n",
    "            output_summary = summarize_docs(docs, output_language=output_language)\n",
    "            \n",
    "            print(output_summary['output_text'])\n",
    "\n",
    "            # Send by telegram\n",
    "\n",
    "            channelId = config['telegram']['channelId']\n",
    "            telegramApiKey = config['telegram']['api_key'] \n",
    "\n",
    "            messageText = f'*{name}* : {transcript[\"title\"]} {transcript[\"published\"]}  \\n' + output_summary['output_text']\n",
    "\n",
    "\n",
    "            for part in divide_text(messageText):\n",
    "                telegramResult = requests.get(\n",
    "                    f\"https://api.telegram.org/bot{telegramApiKey}/sendMessage\",\n",
    "                    params={\"chat_id\": channelId, \"text\": part}\n",
    "                )\n",
    "                print(telegramResult)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
